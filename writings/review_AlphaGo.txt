In this paper, value network (v, evaluate the current board) and policy network (p, estimate the probability of each move under current board) and the integration of v and p in Monte Carlo Tree Search (MCTS) are explained. The result is that AlphaGo being able to dominate all current strongest Ai players and defeat human masters the first in history.

DeepMind applies deep nerual networks (DNN) in building value network and policy network. They first train a DNN to embody the policy network, p1, using data from game records from human experts. DNN enable it to reach an prediction accuracy never achieved before. But at the same time, they develops another policy network, p2, using simpler algorithm than DNN but faster than DNN to produce a prediction. Both p1 and p2 will be used in MCTS, but in different stage.

DeepMind applies DNN also on value network. Once they have p1, they improved it and twik it towards winning game rather than predicting human experts' move by applying reinforcement learning of a DNN. The main idea is to let the p1 and variation of p1 play against each other and learn by winning. The result is p3. The two p3 DNN play with each other and produced a larget dataset that then be used to train a value network. Of course, this value network, v, is also embodied by a DNN.

In gaming, MCTS works by running many simulations. In each simulation, insteading of expanding a tree node to all its children, only on child is selected to expand and then only one of this child's child is expanded...until the game terminates. Then MCTS averages the simulations and pick a child at the beginning.

The variation MCTS that DeepMind uses combines both v, p1 and p2. In one simulation, the first stage of expansion is based on chosing the child with the largest score repetivitly for L levels. This score contains the probability value calculated by p1. Then after reaching L level, v is used to calculate a score of the the board at L and p2, the faster but less accurate policy, is used to simulate the game until termination and returns a score of board at L. Both scores from v and p2 are combined to give a final score of the board at level L. This score is backuped to the parent node, where all backuped values are averaged to get a score the L-1 level. This backing-up happens towards the beginning of this simulation. Then starts an other simulation...the scores are shared between simulations so each simulation brings new info. Finally after running n simulations, the root choose one path that's being choosed most times in simulations.

AlphaGo, even with single-machine hardware, beats all Go Ai players with high winning rates or high give-ups (free moves for opponent). The distributed version then beats single-machine version with another large margin. And finally it becomes the first Ai Go player that beats human masters in the history.